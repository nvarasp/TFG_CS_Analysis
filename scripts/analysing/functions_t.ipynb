{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE WITH THE CREATED FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: unidecode in c:\\users\\usuario\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\usuario\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: nameparser in c:\\users\\usuario\\anaconda3\\lib\\site-packages (1.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install unidecode\n",
    "!pip install wordcloud\n",
    "!pip install nameparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import operator\n",
    "import string\n",
    "import unidecode\n",
    "import nltk.stem as stemmers\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "from wordcloud import WordCloud, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print json objects in a more visual way\n",
    "def jprint(obj):\n",
    "    text = json.dumps(obj, sort_keys=False, indent=4, ensure_ascii=False)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS TO CLEAN THE TEXT\n",
    "def remove_accents(text):\n",
    "    if text:\n",
    "        #remove accents from text\n",
    "        return ' '.join([unidecode.unidecode(w) for w in text.split()])\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_punctuation_marks(text):\n",
    "    if text:\n",
    "        #  Remove the punctuation marks from text \n",
    "        translator = str.maketrans(' ', ' ', string.punctuation)\n",
    "        return(text.translate(translator))\n",
    "        \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def text_to_lower_case(text):\n",
    "    if text:\n",
    "        # convert text to lower case\n",
    "        return text.lower()\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text:\n",
    "        # Remove non ascii text \n",
    "        return text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    if text:\n",
    "        #  remove multiple whitespaces \n",
    "        return(re.sub('\\s+',' ',text))\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_text_marks(text):\n",
    "    if text:\n",
    "        #Removing some web pages characters that are contained in the text extracted from the api\n",
    "        text = re.sub('<p style=\"text-align:justify\">', \"\", text)\n",
    "        text = re.sub(\"<p>\", \"\", text)\n",
    "        text = re.sub(\"</p>\", \"\", text)\n",
    "        text = re.sub(\"<ul>\", \"\", text)\n",
    "        text = re.sub(\"<u>\", \"\", text)\n",
    "        text = re.sub(\"<li>\", \"\", text)\n",
    "        text = re.sub(\"&nbsp\", \"\", text)\n",
    "        text = re.sub(\"&ndash\", \"\", text)\n",
    "        text = re.sub(\"&rsquo;\", \"\", text)\n",
    "        text = re.sub(\"<em>\", \"\", text)\n",
    "        text = re.sub(\"</em>\", \"\", text)\n",
    "        text = re.sub(\"\\n\", \"\", text)\n",
    "        text = re.sub(\"\\r\", \"\", text)\n",
    "        text = re.sub(\"\\t\", \"\", text)\n",
    "        text = re.sub(\"www\", \"\", text)\n",
    "        text = re.sub(\"https\", \"\", text)\n",
    "        #  replace characters like it\\'s by its\n",
    "        text = re.sub(r\"\\'\", \"\", text)\n",
    "        #  replace *, ?, ... by spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def split_text_and_numbers(text):\n",
    "    return text\n",
    "\n",
    "def remove_alone_numbers(text):\n",
    "    if text:\n",
    "        # keep only text\n",
    "        text = re.sub(r\"\\d\", \"\", text)\n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Apply the different functions in order to clean the text\n",
    "    text = text_to_lower_case(text)\n",
    "    text = remove_text_marks(text)\n",
    "    text = remove_punctuation_marks(text)\n",
    "    text = remove_accents(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = split_text_and_numbers(text)\n",
    "    text = remove_alone_numbers(text)\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    \n",
    "    # Return\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_nltk_text(data, \n",
    "                         text_field):\n",
    "    # List that will store tokens\n",
    "    tokens = []\n",
    "    \n",
    "    # Fill up the tokens list with the text comming from data[text_field] \n",
    "    for text in data[text_field].values:\n",
    "        tokens.extend(text.split(\" \"))\n",
    "    \n",
    "    # Return nltk.Text object\n",
    "    return nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    # Get the tokens\n",
    "    if isinstance(text, nltk.Text):\n",
    "        tokens = text.tokens\n",
    "    else:\n",
    "        tokens = text.split(\" \")\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text, \n",
    "                     language):\n",
    "    \n",
    "    # Get stop words for the given language\n",
    "    stopwords_list = stopwords.words(language)\n",
    "    \n",
    "    # Get the tokens\n",
    "    tokens = get_tokens(text)\n",
    "        \n",
    "    # Remove the words from the text\n",
    "    cleaned_text = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Return cleaned text\n",
    "    if isinstance(text, nltk.Text):\n",
    "        output = nltk.Text(cleaned_text)\n",
    "    else:\n",
    "        output = \" \".join(cleaned_text)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def stem_text(text,\n",
    "              language):\n",
    "    #  Create the stemmer\n",
    "    stemmer = stemmers.SnowballStemmer(language)\n",
    "    \n",
    "    # Get the tokens\n",
    "    tokens = get_tokens(text)\n",
    "    \n",
    "    #  Stem each token in text object\n",
    "    stemmas = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Return stemmed text\n",
    "    if isinstance(text, nltk.Text):\n",
    "        output = nltk.Text(stemmas)\n",
    "    else:\n",
    "        output = \" \".join(stemmas)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def standardize_text(text,\n",
    "                     language):\n",
    "    # Remove the stop words\n",
    "    standardized_text = remove_stopwords(text, language)\n",
    "    \n",
    "    # Stem the text\n",
    "    standardized_text = stem_text(standardized_text, language)\n",
    "    \n",
    "    # Return\n",
    "    return standardized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_distribution(dataset: pd.DataFrame,\n",
    "                                  text_field: str):\n",
    "    # compute the vocabulary size for the given text field \n",
    "    vocabulary_size = len(set(dataset[text_field].values))\n",
    "    print(\"\\nThe vocabulary is composed by {0} words \\n\".format(vocabulary_size))\n",
    "    \n",
    "    # compute the lengths\n",
    "    lengths = dataset[text_field].str.split(\" \").str.len().value_counts()\n",
    "    \n",
    "    # Build the figures\n",
    "    plt.figure(figsize = (16, 5))\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    # Plot the distribution\n",
    "    plt.bar(x=lengths.keys(), height=lengths.values)\n",
    "    \n",
    "    # Assign the title\n",
    "    plt.title(\"Distribution of the text length\")\n",
    "    plt.xlabel(\"Text length\")\n",
    "    plt.ylabel(\"Number of initiatives\")\n",
    "\n",
    "    # Set the second plot\n",
    "    plt.subplot(122)\n",
    "    \n",
    "    # plot the box plot\n",
    "    plt.boxplot(x=lengths.keys(), showmeans=True)\n",
    "    \n",
    "    # Show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagofwords(data, attribute, language):\n",
    "\n",
    "    STOPWORDS = set(stopwords.words(language))\n",
    "    \n",
    "    bag_of_words = {}\n",
    "    stemmized_bag={}\n",
    "    for field in data[attribute]:\n",
    "        if field is not None:#to avoid errors if field is None \n",
    "            list_words=list(field.split(\" \"))\n",
    "\n",
    "            for text in list_words:\n",
    "                text=clean_text(text)\n",
    "\n",
    "                # tokenize the text\n",
    "                lst_text = text.split()\n",
    "\n",
    "                # remove stopwords\n",
    "                lst_text = [x for x in lst_text if x not in STOPWORDS]\n",
    "\n",
    "                #  Create the stemmer (tip: see the class nltk.stem)\n",
    "                stemmer = stemmers.SnowballStemmer(language)\n",
    "\n",
    "                #  Stem each token in text object\n",
    "                stemmized = [stemmer.stem(x) for x in lst_text]\n",
    "\n",
    "                # create bag-of-words - for each word the frequency of the word in the corpus\n",
    "                for w in lst_text:\n",
    "                    if w not in bag_of_words:\n",
    "                        bag_of_words[w] = 0\n",
    "                    bag_of_words[w]+=1\n",
    "\n",
    "                for w in stemmized:\n",
    "                    if w not in stemmized_bag:\n",
    "                        stemmized_bag[w] = 0\n",
    "                    stemmized_bag[w]+=1\n",
    "                    \n",
    "                \n",
    "    return bag_of_words, stemmized_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(title, dic_):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "    wordcloud = WordCloud(background_color=\"white\",width=1600, height=800)\n",
    "    wordcloud = wordcloud.generate_from_frequencies(dic_)\n",
    "    ax.axis(\"off\")     \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a barplot\n",
    "def plot_barplot(bag_of_words):\n",
    "    frequent_words=sorted(bag_of_words.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    x, y = zip(*frequent_words[:10])\n",
    "    plt.barh(x,y)\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.title(\"Bar plot of most frequent words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \n",
    "    # Get stop words for the given language\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    regex = r'\\w+'\n",
    "    text = re.findall(regex, data)\n",
    "    # take lowercase and remove stop words\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.text = self.tokenize(data)\n",
    "        self.text_df = self.load_df()\n",
    "        \n",
    "# tokenize raw text into words and then remove stop words\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        regex = r'\\w+'\n",
    "        text = re.findall(regex, data)\n",
    "        return text\n",
    "    \n",
    "# turn list of words into pandas dataframe\n",
    "    def load_df(self):\n",
    "        text_df = pd.DataFrame(self.text, columns=['word'])\n",
    "        text_df = text_df.groupby('word').size().reset_index()\n",
    "        text_df = text_df.rename(columns={0: 'count'})\n",
    "        return text_df\n",
    "    \n",
    "# calculate term frequency\n",
    "    def tf(self):\n",
    "        size = len(self.text)\n",
    "        self.text_df['tf'] = self.text_df['count'] / size\n",
    "            \n",
    "# calculate inverse document frequency\n",
    "    def idf(self, all_text, document_size):\n",
    "        self.text_df['idf'] = self.text_df['word'].apply(lambda word: self.count_idf(word, all_text, document_size))\n",
    "        \n",
    "# helper for idf\n",
    "    def count_idf(self, word, all_text, document_size):\n",
    "        count = 0\n",
    "        for text in all_text:\n",
    "            if word in text:\n",
    "                count = count + 1\n",
    "        return math.log(document_size / (count + 1))\n",
    "    \n",
    "# caculate tfidf together\n",
    "    def tf_idf(self):\n",
    "        self.text_df['tfidf'] = self.text_df['tf'] * self.text_df['idf']\n",
    "        self.text_df = self.text_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_countplot(data, variable, name, percentage):\n",
    "    sns.set(font_scale=1.4)\n",
    "    ax=data[variable].value_counts().plot(kind=\"bar\", figsize=(4,4), rot=0, color=list('rgbkymc'))\n",
    "    total=data[variable].value_counts().sum()\n",
    "    for p in ax.patches:\n",
    "        if percentage==True:\n",
    "            txt = str((p.get_height()/total*100).round(2)) + '%'\n",
    "            txt_x = p.get_x() \n",
    "            txt_y = p.get_height()\n",
    "        else:\n",
    "            txt = str(p.get_height().round(2))\n",
    "            txt_x = p.get_x() \n",
    "            txt_y = p.get_height()\n",
    "        ax.text(txt_x,txt_y,txt)\n",
    "    ax.tick_params(axis='x', labelrotation = 90)\n",
    "    plt.xlabel(variable, labelpad=14)\n",
    "    plt.ylabel(\"Count of \"+name, labelpad=14)\n",
    "    plt.title(\"Count of \"+name+\" \"+variable, y=1.02)\n",
    "    plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_countplot3(data, var1, var2, name, category):\n",
    "    sns.set(font_scale=1.4)\n",
    "    ax=data.plot(var1, var2, kind='bar', figsize=(5,5), color=list('rgbkymc'))\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        txt = str(p.get_height().round(2))\n",
    "        txt_x = p.get_x() \n",
    "        txt_y = p.get_height()\n",
    "        ax.text(txt_x,txt_y,txt)\n",
    "        \n",
    "    ax.tick_params(axis='x', labelrotation = 90)\n",
    "    plt.xlabel(name, labelpad=14)\n",
    "    plt.ylabel(\"Count of \"+category, labelpad=14)\n",
    "    plt.title(\"Count of \"+category+\" \"+name, y=1.02)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function that will be used to plot a variable distinguishing the values of another one.\n",
    "def plot_by_variable(data, var1, var_hue):\n",
    "    \n",
    "    ax = sns.catplot(x=var1, hue=var_hue, kind=\"count\",palette=\"cubehelix\", data=data, height=6, aspect=3)\n",
    "    #ax = sns.catplot(x=var1, hue=var_hue, col_wrap=4, data=data, kind=\"count\", height=2.5, aspect=.8)\n",
    "    ax.set_xticklabels(rotation=45).set_titles(\"{col_name} {col_var}\") \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(pct, allvals):\n",
    "    absolute = int(round(pct/100.*np.sum(allvals)))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "def pie_chart(df, science, count):\n",
    "    labels=science, \"Rest\"\n",
    "    sizes=[count, len(df)-count]\n",
    "    explode=(0, 0.1)\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, explode=explode, labels=labels, autopct=lambda pct: func(pct, sizes), shadow=True, startangle=90)\n",
    "\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie(data, variable, title):\n",
    "    \n",
    "    values = data[variable]\n",
    "    v_counts = data[variable].value_counts()\n",
    "    total = len(values)\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize = (7,7))\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.pie(v_counts, labels=v_counts.index, autopct=lambda pct: func(pct, v_counts));  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with colors\n",
    "def funcc(pct, allvals):\n",
    "    absolute = int(round(pct/100.*np.sum(allvals)))\n",
    "    return \"{:.1f}% ({:d})\".format(pct, absolute)\n",
    "def plot_piee(data, variable, title):\n",
    "    \n",
    "    values = data[variable]\n",
    "    v_counts = data[variable].value_counts()\n",
    "    total = len(values)\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize = (7,7))\n",
    "    plt.title(title)\n",
    "    mycolors = [\"#7FFFD4\", \"#FFE4C4\", \"#5F9EA0\", \"#D2691E\", \"#8FBC8F\"]\n",
    "    plt.pie(v_counts, labels=v_counts.index, colors=mycolors, autopct=lambda pct: funcc(pct, v_counts))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_people(text):\n",
    "    people_list=[]\n",
    "    tokenized = nltk.word_tokenize(text);\n",
    "    tags = nltk.pos_tag(tokenized)\n",
    "    #print(tags)\n",
    "    chunkPattern = r' Chunk0: {<NNP>+<NNP>+}'\n",
    "    chunkParser = nltk.RegexpParser(chunkPattern)\n",
    "    chunkedData = chunkParser.parse(tags)\n",
    "    #print(chunkedData)\n",
    "\n",
    "    for subtree in chunkedData.subtrees(filter=lambda t: t.label() in \"Chunk0\"):\n",
    "        exp = \"\"\n",
    "        for l in subtree.leaves():\n",
    "            exp += str(l[0]) + \" \"\n",
    "        exp = exp[:-1]\n",
    "        people_list.append(exp)\n",
    "\n",
    "    return people_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
