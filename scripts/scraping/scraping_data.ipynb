{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NECESSARY IMPORTS\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables to determine the flow of the code\n",
    "GET_FROM_API=True\n",
    "EXPORT_JSON=True\n",
    "SCRAP_SPANISH=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print json objects in a more visual way\n",
    "def jprint(obj):\n",
    "    text = json.dumps(obj, sort_keys=False, indent=4, ensure_ascii=False)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Spanish platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "    #Import the google chrome driver \n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    \n",
    "    #go to the platform's webpage\n",
    "    driver.get('https://ciencia-ciudadana.es/proyecto-cc/')\n",
    "    \n",
    "    #Get all the projects titles and links and store them in a list\n",
    "    projects = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "\n",
    "    projects_list=[]\n",
    "\n",
    "    for project in projects:\n",
    "        title=project.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "        link = title.get_attribute('href')\n",
    "\n",
    "        #print(title.text)\n",
    "        #print(link)\n",
    "\n",
    "        this_project={\"Title\": title.text, \"Link\": link}\n",
    "        projects_list.append(this_project)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "    \n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    \n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios=[]\n",
    "\n",
    "    #Iterate, for every project in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(projects_list)):\n",
    "        project_dict={}\n",
    "\n",
    "        driver.get(projects_list[i][\"Link\"])\n",
    "        left_area=driver.find_element_by_xpath(\"//*[@id='left-area']\")\n",
    "        idd=left_area.find_element_by_xpath(\".//*\").get_attribute(\"id\")\n",
    "\n",
    "        project_area=left_area.find_element_by_xpath(\"//*/div[1]/h1\")\n",
    "        project_dict[\"id\"]=idd\n",
    "        project_dict[\"name\"]=project_area.text\n",
    "        project_dict[\"type\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[3]\").text\n",
    "        project_dict[\"aim\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[1]\").text\n",
    "        project_dict[\"subtitle\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[2]\").text\n",
    "        project_dict[\"description\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[2]\").text\n",
    "        project_dict[\"keywords\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[4]\").text\n",
    "        project_dict[\"start_date\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[1]/div[2]/div[1]\").text\n",
    "        project_dict[\"end_date\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[1]/div[2]/div[2]\").text\n",
    "        project_dict[\"url\"]=projects_list[i][\"Link\"]\n",
    "        project_dict[\"mainOrganisation\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[1]\").text\n",
    "        project_dict[\"responsibleEntity\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[3]\").text\n",
    "        project_dict[\"province\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[1]/div[2]/div[4]\").text\n",
    "        project_dict[\"howToParticipate\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[6]\").text\n",
    "        project_dict[\"equipment\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[7]\").text\n",
    "        project_dict[\"public\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[1]/div[2]/div[3]\").text\n",
    "        project_dict[\"participants\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[1]/div[2]/div[5]\").text\n",
    "        project_dict[\"fundingTeam\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[4]\").text\n",
    "        project_dict[\"moreEntities\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[5]/div[5]\").text\n",
    "        project_dict[\"results_summary\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[7]/div[1]\").text\n",
    "        project_dict[\"results_link\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[7]/div[2]\").text\n",
    "        project_dict[\"impact\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[7]/div[3]\").text\n",
    "        project_dict[\"impact_examples\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[7]/div[4]\").text\n",
    "        project_dict[\"motivation\"]=left_area.find_element_by_xpath(\"//*/div[2]/div/div/div[2]/div[9]/div\").text\n",
    "\n",
    "        lista_diccionarios.append(project_dict)\n",
    "    driver.close()\n",
    "    \n",
    "    '''print(\"Number of projects: \",len(projects_list))\n",
    "    jprint(lista_diccionarios)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP_projects=pd.DataFrame.from_dict(lista_diccionarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishInitiatives.json', 'w', encoding='utf-8') as fp:\n",
    "        df_SP_projects.to_json(fp, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishPeople.json', 'w', encoding='utf-8') as fp:\n",
    "        df_SP_projects[df_SP_projects[\"type\"]==\"persona de ciencia ciudadana\"].to_json(fp, force_ascii=False)\n",
    "        \n",
    "    with open('SpanishInstitutions.json', 'w', encoding='utf-8') as fp:\n",
    "        df_SP_projects[df_SP_projects[\"type\"]==\"instituci√≥n de ciencia ciudadana\"].to_json(fp, force_ascii=False)\n",
    "    \n",
    "    with open('SpanishProjects.json', 'w', encoding='utf-8') as fp:\n",
    "        df_SP_projects[df_SP_projects[\"type\"]==\"proyecto de ciencia ciudadana\"].to_json(fp, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDUCATIONAL RESOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    #Set up the path to the chrome driver\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    driver.get('https://ciencia-ciudadana.es/category/recursos_educativos/')\n",
    "\n",
    "    #Get all the educational resources names and links and store them in a list\n",
    "    educational_resources = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "    \n",
    "    educational_resources_list=[]\n",
    "\n",
    "    for er in educational_resources:\n",
    "        title=er.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "        link = title.get_attribute('href')\n",
    "\n",
    "        this_er={\"Title\": title.text, \"Link\": link}\n",
    "        educational_resources_list.append(this_er)\n",
    "        \n",
    "    #Since there is more than one page with educational resources\n",
    "    num_pages = driver.find_elements_by_xpath('//a[@class=\"wpv-archive-pagination-link js-wpv-archive-pagination-link page-link\"]')\n",
    "        \n",
    "    links_pages=[]\n",
    "    for page in num_pages:\n",
    "        links_pages.append(page.get_attribute(\"href\"))\n",
    "        \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://ciencia-ciudadana.es/category/recursos_educativos/page/2/']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "    \n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "#Go to the next page with educational resources and do the same\n",
    "    for i in range(len(links_pages)):\n",
    "        driver.get(links_pages[i])\n",
    "        #Get all the educational resources names and links and store them in a list\n",
    "        educational_resources = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "\n",
    "        for er in educational_resources:\n",
    "            title=er.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "            link = title.get_attribute('href')\n",
    "\n",
    "            this_er={\"Title\": title.text, \"Link\": link}\n",
    "            educational_resources_list.append(this_er)\n",
    "    #educational_resources_list\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "\n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios_er=[]\n",
    "\n",
    "    #Iterate, for every resource in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(educational_resources_list)):\n",
    "        educational_resource_dict={}\n",
    "\n",
    "        driver.get(educational_resources_list[i][\"Link\"])\n",
    "\n",
    "        educational_resource_dict[\"name\"]=educational_resources_list[i][\"Title\"]        \n",
    "        educational_resource_dict[\"url\"]=educational_resources_list[i][\"Link\"]\n",
    "\n",
    "        content=driver.find_element_by_xpath(\"//div[@id='left-area']\")\n",
    "\n",
    "        text_list=[]\n",
    "        sentences = content.find_elements_by_xpath(\"//p\")\n",
    "    \n",
    "        for i in range(len(sentences)-4):\n",
    "            if(sentences[i].text!=\"\"):\n",
    "                text_list.append(sentences[i].text)\n",
    "\n",
    "        keywords=[]\n",
    "        strong=driver.find_elements_by_xpath(\"//strong\")\n",
    "        for negrita in strong:\n",
    "            keywords.append(negrita.text)\n",
    "            \n",
    "        educational_resource_dict[\"abstract\"]=text_list\n",
    "        educational_resource_dict[\"keywords\"]=keywords\n",
    "        \n",
    "        lista_diccionarios_er.append(educational_resource_dict)\n",
    "    driver.close()\n",
    "    #lista_diccionarios_er\n",
    "    #jprint(lista_diccionarios_er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishEducationalResources.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(lista_diccionarios_er, fp,  ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFORMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    #Set up the path to the chrome driver\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    driver.get('https://ciencia-ciudadana.es/category/informes/')\n",
    "\n",
    "    #Get all the names and links, and store them in a list\n",
    "    informes = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "    \n",
    "    informes_list=[]\n",
    "\n",
    "    for informe in informes:\n",
    "        title=informe.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "        link = title.get_attribute('href')\n",
    "\n",
    "        this_informe={\"Title\": title.text, \"Link\": link}\n",
    "        informes_list.append(this_informe)\n",
    "        \n",
    "    #Since there is more than one page   \n",
    "    num_pages = driver.find_elements_by_xpath('//a[@class=\"wpv-archive-pagination-link js-wpv-archive-pagination-link page-link\"]')\n",
    "    \n",
    "    links_pages=[]\n",
    "    for page in num_pages:\n",
    "        links_pages.append(page.get_attribute(\"href\"))\n",
    "    links_pages\n",
    "    \n",
    "    #informes_list\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    #go to the next pages and do the same\n",
    "    for i in range(len(links_pages)):\n",
    "        driver.get(links_pages[i])\n",
    "        #Get all the reports' names and links and store them in a list\n",
    "        informes = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "\n",
    "        for informe in informes:\n",
    "            title=informe.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "            link = title.get_attribute('href')\n",
    "\n",
    "            this_informe={\"Title\": title.text, \"Link\": link}\n",
    "            informes_list.append(this_informe)\n",
    "    driver.close()\n",
    "    #informes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "\n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios_informes=[]\n",
    "\n",
    "    #Iterate, for every report in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(informes_list)):\n",
    "        informes_dict={}\n",
    "\n",
    "        driver.get(informes_list[i][\"Link\"])\n",
    "\n",
    "        informes_dict[\"name\"]=informes_list[i][\"Title\"]\n",
    "        informes_dict[\"url\"]=informes_list[i][\"Link\"]\n",
    "        \n",
    "        content=driver.find_element_by_xpath(\"//div[@class='entry-content']\")\n",
    "\n",
    "        text_list=[]\n",
    "        sentences = content.find_elements_by_xpath(\"//p\")\n",
    "        \n",
    "        for i in range(len(sentences)-4):\n",
    "            if(sentences[i].text!=\"\"):\n",
    "                text_list.append(sentences[i].text)\n",
    "\n",
    "        keywords=[]\n",
    "        strong=driver.find_elements_by_xpath(\"//strong\")\n",
    "        for negrita in strong:\n",
    "            keywords.append(negrita.text)\n",
    "            \n",
    "        informes_dict[\"abstract\"]=text_list\n",
    "        informes_dict[\"keywords\"]=keywords\n",
    "\n",
    "        lista_diccionarios_informes.append(informes_dict)\n",
    "        \n",
    "    driver.close()\n",
    "\n",
    "    #lista_diccionarios_informes\n",
    "    #jprint(lista_diccionarios_informes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishReports.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(lista_diccionarios_informes, fp,  ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFOGRAFIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    #Set up the path to the chrome driver\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    driver.get('https://ciencia-ciudadana.es/category/infografias/')\n",
    "\n",
    "    #Get all the names and links and store them in a list\n",
    "    infografias = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "    \n",
    "    infografias_list=[]\n",
    "\n",
    "    for infografia in infografias:\n",
    "        title=infografia.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "        link = title.get_attribute('href')\n",
    "\n",
    "        this_infografia={\"Title\": title.text, \"Link\": link}\n",
    "        infografias_list.append(this_infografia)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "\n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios_infografias=[]\n",
    "\n",
    "    #Iterate, for every element in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(infografias_list)):\n",
    "        infografias_dict={}\n",
    "\n",
    "        driver.get(infografias_list[i][\"Link\"])\n",
    "\n",
    "        infografias_dict[\"name\"]=infografias_list[i][\"Title\"]\n",
    "        infografias_dict[\"url\"]=infografias_list[i][\"Link\"]\n",
    "        \n",
    "        content=driver.find_element_by_xpath(\"//div[@class='entry-content']\")\n",
    "\n",
    "        keywords=[]\n",
    "        strong=driver.find_elements_by_xpath(\"//strong\")\n",
    "        for negrita in strong:\n",
    "            keywords.append(negrita.text)\n",
    "        infografias_dict[\"keywords\"]=keywords\n",
    "        \n",
    "        text_list=[]\n",
    "        sentences = content.find_elements_by_xpath(\"//p\")\n",
    "        \n",
    "        for i in range(len(sentences)-4):\n",
    "            if(sentences[i].text!=\"\"):\n",
    "                text_list.append(sentences[i].text)\n",
    "        infografias_dict[\"abstract\"]=text_list\n",
    "\n",
    "        lista_diccionarios_infografias.append(infografias_dict)\n",
    "        \n",
    "    driver.close()\n",
    "    #jprint(lista_diccionarios_infografias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishInfographics.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(lista_diccionarios_infografias, fp,  ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GU√çAS Y METODOLOG√çAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    #Set up the path to the chrome driver\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    driver.get('https://ciencia-ciudadana.es/category/guias-y-metodologias/')\n",
    "\n",
    "    #Get all the elements' names and links and store them in a list\n",
    "    guias = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "    \n",
    "    guias_list=[]\n",
    "\n",
    "    for guia in guias:\n",
    "        title=guia.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "        link = title.get_attribute('href')\n",
    "\n",
    "        this_guia={\"Title\": title.text, \"Link\": link}\n",
    "        guias_list.append(this_guia)\n",
    "        \n",
    "    #Since there is more than one page\n",
    "    num_pages = driver.find_elements_by_xpath('//a[@class=\"wpv-archive-pagination-link js-wpv-archive-pagination-link page-link\"]')\n",
    "   \n",
    "    links_pages=[]\n",
    "    for page in num_pages:\n",
    "        links_pages.append(page.get_attribute(\"href\"))\n",
    "        \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    for i in range(len(links_pages)):\n",
    "        driver.get(links_pages[i])\n",
    "        \n",
    "        #Get all the elements' names and links and store them in a list\n",
    "        guias = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "\n",
    "        for guia in guias:\n",
    "            title=guia.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "            link = title.get_attribute('href')\n",
    "\n",
    "            this_guia={\"Title\": title.text, \"Link\": link}\n",
    "            guias_list.append(this_guia)\n",
    "            \n",
    "    driver.close()\n",
    "    guias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "\n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios_guias=[]\n",
    "\n",
    "    #Iterate, for every element in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(guias_list)):#len(educational_resources_list)\n",
    "        guias_dict={}\n",
    "\n",
    "        \n",
    "        driver.get(guias_list[i][\"Link\"])\n",
    "\n",
    "        guias_dict[\"name\"]=guias_list[i][\"Title\"]\n",
    "        guias_dict[\"url\"]=guias_list[i][\"Link\"]\n",
    "        \n",
    "        content=driver.find_element_by_xpath(\"//div[@class='entry-content']\")\n",
    "\n",
    "        keywords=[]\n",
    "        strong=driver.find_elements_by_xpath(\"//strong\")\n",
    "        for negrita in strong:\n",
    "            keywords.append(negrita.text)\n",
    "        guias_dict[\"keywords\"]=keywords\n",
    "\n",
    "        text_list=[]\n",
    "        sentences = content.find_elements_by_xpath(\"//p\")\n",
    "\n",
    "        for i in range(len(sentences)-4):\n",
    "            if(sentences[i].text!=\"\"):\n",
    "                text_list.append(sentences[i].text)\n",
    "        guias_dict[\"abstract\"]=text_list\n",
    "\n",
    "        lista_diccionarios_guias.append(guias_dict)\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "    #jprint(lista_diccionarios_guias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishGuides.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(lista_diccionarios_guias, fp,  ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√âTRICAS E INDICADORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    #Set up the path to the chrome driver\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    driver.get('https://ciencia-ciudadana.es/category/metricas-e-indicadores/')\n",
    "\n",
    "    #Get all the elements' names and links and store them in a list\n",
    "    metricas = driver.find_elements_by_xpath('//div[@class=\"tb-grid-column\"]')\n",
    "    \n",
    "    metricas_list=[]\n",
    "\n",
    "    for metrica in metricas:\n",
    "        title=metrica.find_element_by_xpath('.//h4[@class=\"tb-heading has-text-color\"]/a')#.text\n",
    "        link = title.get_attribute('href')\n",
    "\n",
    "        this_metrica={\"Title\": title.text, \"Link\": link}\n",
    "        metricas_list.append(this_metrica)\n",
    "        \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_SPANISH):\n",
    "\n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "\n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios_metricas=[]\n",
    "\n",
    "    #Iterate, for every element in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(metricas_list)):#len(educational_resources_list)\n",
    "        metricas_dict={}\n",
    "\n",
    "        driver.get(metricas_list[i][\"Link\"])\n",
    "\n",
    "        metricas_dict[\"name\"]=metricas_list[i][\"Title\"]\n",
    "        metricas_dict[\"url\"]=metricas_list[i][\"Link\"]\n",
    "        \n",
    "        content=driver.find_element_by_xpath(\"//div[@class='entry-content']\")\n",
    "\n",
    "        keywords=[]\n",
    "        strong=driver.find_elements_by_xpath(\"//strong\")\n",
    "        for negrita in strong:\n",
    "            keywords.append(negrita.text) \n",
    "        metricas_dict[\"keywords\"]=keywords\n",
    "\n",
    "        text_list=[]\n",
    "        sentences = content.find_elements_by_xpath(\"//p\")\n",
    "\n",
    "        for i in range(len(sentences)-4):\n",
    "            if(sentences[i].text!=\"\"):\n",
    "                text_list.append(sentences[i].text)\n",
    "        metricas_dict[\"abstract\"]=text_list\n",
    "\n",
    "        lista_diccionarios_metricas.append(metricas_dict)\n",
    "        \n",
    "    driver.close()\n",
    "    #jprint(lista_diccionarios_metricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (EXPORT_JSON):\n",
    "    with open('SpanishMetrics.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(lista_diccionarios_metricas, fp,  ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barcelona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAP_BCN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_BCN):\n",
    "    #Import the google chrome driver \n",
    "    driver= webdriver.Chrome('./chromedriver.exe')\n",
    "    \n",
    "    bcn_projects_list=[]\n",
    "\n",
    "    driver.get('https://www.barcelona.cat/barcelonaciencia/es/proyectos-ciencia-ciudadana')\n",
    "    \n",
    "    #Get all the projects titles and links and store them in a list\n",
    "    titles=driver.find_elements_by_xpath('//a[@class=\"color-black\"]')\n",
    "    for title in titles:\n",
    "        this_project={\"Title\": title.text, \"Link\": title.get_attribute('href')}\n",
    "        bcn_projects_list.append(this_project)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SCRAP_BCN):\n",
    "\n",
    "    #Create a list of dictionaries\n",
    "    lista_diccionarios=[]\n",
    "\n",
    "    #Iterate, for every project in the list: store the desired fields in a dictionary and append the \n",
    "    #dictionary to the lista_diccionarios\n",
    "    for i in range(len(bcn_projects_list)):\n",
    "        project_dict={}\n",
    "\n",
    "        driver.get(bcn_projects_list[i][\"Link\"])\n",
    "        project_dict[\"name\"]=bcn_projects_list[i][\"Title\"]\n",
    "\n",
    "        left_area=driver.find_element_by_xpath(\"//*[@id='block-system-main']/div[2]/div/div[1]\")\n",
    "        project_dict[\"description\"]=left_area.find_element_by_xpath(\".//p[2]\").text\n",
    "\n",
    "        #Since for some cases the strong expression is a little bit different (includes the white space or not..), these try and except\n",
    "        #manage to get the fields for all cases, and if they are empty they are set to null. \n",
    "        try:\n",
    "            try:\n",
    "                text = driver.find_element_by_xpath(\"//p[strong = 'Estado:']\").text.strip()\n",
    "            except:\n",
    "                text = driver.find_element_by_xpath(\"//p[strong = 'Estado: ']\").text.strip()\n",
    "            project_dict[\"status\"] = text.split(\":\")[-1].strip()\n",
    "        except:\n",
    "            project_dict[\"status\"]=\"\"\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                text = driver.find_element_by_xpath(\"//p[strong = 'Actividades en el marco de la Oficina: ']\").text.strip()\n",
    "            except:\n",
    "                text = driver.find_element_by_xpath(\"//p[strong = 'Actividades en el marco de la Oficina:']\").text.strip()\n",
    "        except:\n",
    "            text = driver.find_element_by_xpath(\"//p[strong = 'Actividades en el marco de la Oficina']\").text.strip()\n",
    "        project_dict[\"activities\"]=  text.split(\":\")[-1].strip() \n",
    "\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                try:\n",
    "                    text = driver.find_element_by_xpath(\"//p[strong = '√Åmbito: ']\").text.strip()\n",
    "                except:\n",
    "                    text = driver.find_element_by_xpath(\"//p[strong = '√Åmbito:']\").text.strip()\n",
    "            except:\n",
    "                    text = driver.find_element_by_xpath(\"//p[strong = 'mbito:']\").text.strip()\n",
    "            project_dict[\"topic\"]=text.split(\":\")[-1].strip()\n",
    "        except: project_dict[\"topic\"]=\"\" \n",
    "\n",
    "        project_dict[\"bcn_url\"]=bcn_projects_list[i][\"Link\"]\n",
    "        project_dict[\"official_url\"]=driver.find_element_by_xpath(\"//div[@class='field-item even']/a\").get_attribute('href')\n",
    "\n",
    "        lista_diccionarios.append(project_dict)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BarcelonianProjects.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(lista_diccionarios, fp,  ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
